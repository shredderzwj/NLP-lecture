00:21:15	陶开:	没声音
00:22:29	徐楠:	1
00:22:31	陈悦:	1
00:22:31	匡鸿深:	1
00:22:32	卢宇宙:	1
00:22:32	张培栋:	1
00:22:34	孟乒乒:	1
00:22:35	夏敏:	1
00:22:35	薛毓铨:	可以
00:22:35	陶开:	有杂音
00:22:38	旭生 孙:	111
00:23:32	邓 奇:	你们能听到声音？
00:23:37	Brian:	可以
00:23:38	houzhaoding:	可以
00:23:45	潘力:	可以
00:23:59	邓 奇:	嗯 可以了
00:23:59	k:	老师以后上课把视频都统一关掉
00:24:01	k:	吧
00:30:22	王健:	有声音吗
00:30:26	王健:	没声音a
00:30:29	邓 奇:	有
00:30:36	樊建昌:	有
00:30:39	樊建昌:	没有
00:30:43	王健:	为啥我啥都听不到
00:30:44	包颖:	老师请问是大项目才是小组做的。两个小项目是个人做的么？
00:31:40	Brian:	请问综述有格式样本么？
00:31:56	陈靖韦:	小组的大项目是自己找吗
00:32:16	匡鸿深:	项目的数据不好弄
00:32:27	林汉斌:	后面的专业课没项目吗
00:32:42	包颖:	老师请问最后一周汇报是可以汇报大项目或者小项目都可以是么？
00:32:58	严广宇:	大项目必须要做成应用展示吗？可以不做后台和数据库部分吗？
00:33:52	严广宇:	明白了
00:38:41	陈智杰:	轻量级吗。。。。
00:54:06	樊建昌:	过
00:54:06	陈智杰:	没有
00:54:20	张培栋:	pass
00:55:09	小天-king-北京-1年:	5层网络？
00:55:28	何川:	1
00:55:59	王健:	6层吧
00:56:20	小天-king-北京-1年:	6层，刚才对话框挡住了。
00:56:31	华涓 陈:	input-convolution-pooling-convolution--pooling-hidden layer-output？
00:56:33	彭冰:	反正 4个卷积层 两个全连接层吧 分不清这个到底是怎么计算的
00:56:42	Eric:	怎么跟右上角的数据不一样？
00:56:44	小天-king-北京-1年:	FC（120）是什么呢？
00:56:50	王健:	全连接层
00:56:57	王健:	铺开120
00:57:04	王健:	最后10分类
00:57:16	王健:	flatten
00:57:22	Brian:	1
00:57:31	孟乒乒:	flatten也算一层吗
00:57:43	王健:	不算
00:57:46	王健:	就是个操作
00:57:50	樊建昌:	flatten不是，是把数据拉直
00:57:53	王健:	转成1维
00:57:59	张磊:	fully connect layer
00:58:15	张磊:	connected
00:59:37	Brian:	10 的话 算不算channel？
00:59:38	陈韵莹:	FC前有Flatten操作和将3维的数据直接FC有区别吗？
00:59:46	k:	老师大概讲下这个过程ba
00:59:48	qinzongding:	0基础，看不懂，
00:59:48	齐鹏飞:	右上角@前面的也是channel吗
00:59:52	孟明:	flatten之后应该是50*4*4=800个吧，为什么只有500个了？
01:00:00	Eric:	右上角的输入为什么写的是32x32？
01:00:06	匡鸿深:	Hidder units代表什么意义
01:00:20	小天-king-北京-1年:	subsampling 啥意思这啊？
01:00:20	王健:	从头过一下吧
01:00:24	k:	对
01:00:26	Brian:	好的
01:00:26	k:	过一下
01:00:33	陈洪剑:	以前学过，又忘了
01:00:50	Brian:	3 channel
01:00:57	陈洪剑:	3通道
01:01:31	齐鹏飞:	右上角的Layer1 的6@28x28是？
01:01:34	Eric:	怎么搞的呢？
01:02:26	张培栋:	为啥20
01:02:40	刘向宏:	多用几个核
01:02:41	马玄:	20个kenel
01:02:44	王奕楷:	20是自己定的
01:02:53	小天-king-北京-1年:	输入时3层，变成20层，就是使用了很多个卷积核把？
01:03:02	王奕楷:	lenet的设定就是20个
01:03:03	潘力:	每一个kernel都会产生一个feature map 吗？
01:03:04	张培栋:	28-5+1
01:03:05	王健:	20 . 24 .24
01:03:14	王健:	20个卷积核
01:03:25	陈智杰:	kernel的数量
01:03:25	潘力:	懂了
01:03:26	Brian:	是kernel的数量决定的吧？
01:03:27	王健:	增加卷积核提取特征
01:03:27	吴彬:	en
01:03:32	张培栋:	ok
01:03:53	qinzongding:	20 个5*5kernal 去卷吗
01:04:25	王健:	50 . 5 . 5
01:04:33	Brian:	后面以此类推
01:04:37	王健:	去卷积
01:04:46	张培栋:	ok
01:04:57	王健:	增加卷积核提取更多的特征
01:04:57	吴彬:	1
01:05:26	旭生 孙:	Ok
01:05:51	qinzongding:	怎么用fc变到500units 的？
01:06:07	王健:	老师卷积核是后面越来越多吗
01:06:21	张磊:	800与500的全连接？
01:06:31	马玄:	中间20*12*12 怎么变到50*8*8？
01:06:31	樊建昌:	不是啊
01:06:35	Brian:	看录像
01:06:38	樊建昌:	reshape就行
01:06:43	小天-king-北京-1年:	3@28x28 到 20@24x24是几个卷积核呢？20个的话？那就是输入的3层变成了一层？
01:06:53	王健:	老师，每个卷积核都什么规则啊
01:07:24	樊建昌:	老师，这个卷积核就是一开始的参数初始化的是吧？
01:07:38	王健:	例如第一层20个卷积核，这20个都有什么规律呢
01:07:50	齐鹏飞:	应该是20个5*5*3的kernel
01:08:07	王健:	例如什么边缘，锐化，模糊啥的
01:08:48	Eric:	第三层是50个卷积核吗？
01:08:58	houzhaoding:	卷积核的大小如何确定？自己定义吗？
01:08:59	王健:	哦哦，卷积核是反向传播参数来的啊
01:09:03	王健:	才明白
01:09:17	王健:	以为最开始就设定完了
01:09:17	匡鸿深:	由于每层的卷积目标都不同，每层的卷积核设计没有限制之类的吗
01:09:26	王健:	数据来影响卷积核
01:09:30	彭冰:	每一个卷积核代表了提取图片中一种特征的方法 20个卷积核就是在这一层选择找到20个特征 至于卷积核的参数 交给网络自己学习
01:09:31	樊建昌:	老师，我问下
01:09:38	樊建昌:	好
01:09:42	樊建昌:	好的好的
01:09:42	齐鹏飞:	1
01:09:45	王健:	1
01:09:48	贾强汉:	OK
01:09:52	张磊:	1
01:10:37	徐楠:	分开运算，当时硬件不满足吧
01:10:46	王健:	分布式
01:11:14	徐楠:	48
01:11:15	唐陈:	48
01:11:15	陈智杰:	48
01:11:16	张磊:	96
01:11:16	小天-king-北京-1年:	48额
01:11:17	夏敏:	48
01:11:17	罗家伟:	48
01:11:19	卢宇宙:	48
01:11:19	旭生 孙:	48
01:12:25	Eric:	我怎么觉得是96？
01:12:40	王健:	各自48 ，肯定还是48
01:12:46	徐楠:	老师☞了第一层
01:13:45	Eric:	也就是换了一下kernel的大小，然后发了论文吗？
01:14:51	王健:	bn
01:15:11	王健:	后面好像就加了个bn
01:15:23	Brian:	这些经典网络要掌握到什么程度？
01:15:57	Eric:	也就是换了一下Kernel的大小，然后就发了论文吗？
01:16:03	WW BB:	9:05
01:16:10	樊建昌:	老师，我想要问下，就是刚开始说的那个卷积核的初始化，各种初始化方法好像对最后的训练的结果有影响吧？
01:16:21	Eric:	哦哦，谢谢。
01:16:34	樊建昌:	我看的资料对于不同的激活函数好像初始化方法不同？
01:16:35	houzhaoding:	老师能抽空讲讲新出来的与百度合作的飞浆项目吗？
01:16:42	小天-king-北京-1年:	我还想问一下刚才的问题，3@28x28 到 20@24x24是几个卷积核呢？它的收入是3层，通过卷积得到20个层。
01:16:51	樊建昌:	好的，谢谢老师。
01:16:53	Eric:	能把PPT回拨到第二个网络吗？
01:16:56	华涓 陈:	刚刚走了下神。。输入是3个channel，卷积核是20，那3个channel各自分别跟20个核计算计算是么？
01:17:14	王健:	那个太贵l
01:17:18	王健:	了
01:17:23	匡鸿深:	老师，感觉隐藏层都是做卷积计算，而卷积核又都是学习得到的，干嘛不直接就堆这些层就行了，还做这么多区分
01:17:40	王健:	飞桨很贵啊
01:17:56	樊建昌:	钱不是主要问题，主要是能掌握啊
01:18:04	Eric:	这个不是 96@55x55吗？
01:18:10	Brian:	钱不是主要问题。。
01:18:15	陈洪剑:	paddle框架推广活动
01:18:28	樊建昌:	制作数据集要命啊
01:18:29	王健:	啊，老师，咱们上那个多少钱啊
01:18:32	樊建昌:	烦死人
01:18:37	侠航 陈:	钱一个月就赚回来了
01:18:43	高格格:	老师，之后GAN的限选课也会有项目吗？
01:18:44	张天虎:	感觉，理论上，pooling层可以被conv层替代吧，为什么不那么做呢？
01:18:45	樊建昌:	不要在乎钱啊
01:18:47	Eric:	这个网络不是96@55x55吗？
01:19:23	高格格:	好的~
01:19:26	旭生 孙:	讲讲人脸转正吧 特别想知道
01:19:45	王健:	老师咱们班上飞桨那个多少钱啊
01:19:52	齐鹏飞:	上下两部分
01:20:18	齐鹏飞:	paddle 3w多
01:20:55	Eric:	现代的CNN也是人为设定Kernel吗？
01:20:57	陈洪剑:	对抗式生成网络GAN?
01:21:31	Eric:	不是，我是说Kernel的数目呀！
01:21:45	Eric:	不是Kernel的值。
01:21:52	Eric:	哦哦，谢谢老师。
01:22:16	王健:	飞桨我还是算了，好好上这个了就
01:23:13	张培栋:	Kernel数目，和BP学来的内容有关系吗？
01:23:21	Brian:	不知道飞桨和这个有没有重复的内容
01:23:24	王健:	数目自定
01:23:42	王健:	但最后kerner里面的矩阵具体数值是bp来更新的
01:23:52	王健:	应该是这样 ，不知道有没有说错
01:23:57	张培栋:	数目自定，但是感觉也得考虑其他因素吧
01:24:22	王健:	数目这个自定，但有经验值参考
01:24:57	Brian:	1
01:24:58	潘力:	1
01:24:58	邓 奇:	1
01:25:00	齐鹏飞:	1
01:25:01	张磊:	1
01:25:02	严广宇:	1
01:25:02	旭生 孙:	1
01:25:02	刘向宏:	1
01:25:40	陈洪剑:	炼丹
01:25:44	Brian:	凭感觉
01:25:46	齐鹏飞:	炼丹
01:26:01	张培栋:	还是经验，哈哈
01:26:01	Eric:	有没有研究自动生成Kernel数目的方法？
01:26:03	孟明:	flatten并不是把所有像素排成一排，而是通过与图像大小相同的kernel卷积完成的？
01:26:13	齐鹏飞:	automl
01:26:17	王健:	就是铺平
01:26:20	王奕楷:	哪有什么经验，试就完了
01:26:23	王健:	没那么麻烦
01:26:42	王健:	肯定有经验啊，试那得有多少可能啊
01:27:03	王健:	本身cv就偏实验，理论性很弱
01:27:46	王健:	我记得前几天有篇文章就是gan的训练经验说的都是一些实践技巧
01:27:52	旭生 孙:	不明白
01:28:01	Brian:	大概讲讲
01:28:13	Eric:	感觉跟传统的网络没有区别呀。
01:28:14	潘力:	倒数第二个pooling是不是应该是1024？
01:28:18	樊建昌:	理论很强的
01:28:25	樊建昌:	数学要求很高的
01:28:33	小天-king-北京-1年:	不同的网络，使用不同的框架把？tensorflow的网络不能跑在pytroch上吧？
01:28:46	Brian:	GAN？
01:28:50	潘力:	256->512->不是1024吗
01:28:53	华涓 陈:	前面的卷积核数量都是翻倍的，后面就不翻倍的是因为过拟合么，还是固定这样的？
01:28:54	陈智杰:	长宽越来越短，channel越来越多了
01:28:55	齐鹏飞:	网络是共通的
01:29:09	Brian:	kernel
01:29:27	陈韵莹:	64
01:29:27	陈智杰:	64
01:29:27	邓 奇:	64
01:29:28	夏敏:	64
01:29:29	小天-king-北京-1年:	64s64
01:29:30	旭生 孙:	64
01:29:31	徐楠:	64
01:29:31	陈洪剑:	64
01:31:00	旭生 孙:	1/2 pool的意义是什么呢
01:31:25	王健:	方便计算
01:31:32	张磊:	计算量小
01:31:34	王健:	计算量小些
01:31:38	齐鹏飞:	参数少
01:31:40	陈韵莹:	参数少
01:32:00	小天-king-北京-1年:	估计这些他都测试过，3最好
01:32:28	严广宇:	连续多个3*3的卷积，与单个大卷积核的感受野相同。但具有更少的参数，可以提供更多的非线性变化
01:33:01	樊建昌:	不好
01:33:03	樊建昌:	计算量大
01:34:18	陈洪剑:	时间复杂度降低？
01:34:29	严广宇:	1
01:34:31	齐鹏飞:	1
01:34:31	陈智杰:	这里为啥是拿3和7比？
01:34:34	陈智杰:	不是3和5比？
01:34:43	李文艺:	都可以
01:34:50	李文艺:	只是个例子
01:34:51	陈洪剑:	5也可以比较吧
01:34:53	徐楠:	一样的比法
01:35:19	李文艺:	我今天写总结也写到了这个
01:35:23	潘力:	减少sise
01:35:27	潘力:	size
01:35:34	张培栋:	融合特征
01:35:40	陈洪剑:	降低计算量
01:36:37	Eric:	pooling不就是局部锐化吗？
01:36:54	王健:	不一定
01:37:05	王健:	也有avgpooling
01:37:27	Eric:	哪个网络呀？
01:37:54	王健:	caffe
01:38:10	张磊:	大神
01:38:15	Brian:	华人
01:38:19	陈洪剑:	真是大神
01:38:24	王健:	清华的
01:38:32	张磊:	看不清啊
01:38:35	张磊:	哈哈
01:38:40	樊建昌:	哈哈
01:38:44	Brian:	。。。哈哈
01:38:55	王健:	好多cv大佬都是清华出来的
01:39:02	Eric:	Norm具体是做什么？
01:39:08	齐鹏飞:	可以将cv去掉
01:39:17	Brian:	Concat？
01:39:21	Brian:	是说明
01:39:23	Brian:	是什么
01:39:32	樊建昌:	拼接
01:39:36	樊建昌:	直接拼接
01:39:40	Brian:	ok
01:39:44	Baoce SUN:	1*1
01:39:53	张培栋:	没有全连接吗？
01:40:00	陈洪剑:	正则？
01:40:04	孟明:	归一化？
01:40:29	王健:	全是bn了
01:42:02	Brian:	拼接主要是长度变化？？
01:42:10	Eric:	那为什么不直接用150个Kernel呢？
01:42:46	华涓 陈:	但是拼接前不会出现一个是28*28*50，一个是26*26*1000么
01:43:18	李文艺:	降维加速，线性变换，通道整合
01:43:31	高格格:	针
01:45:58	Eric:	那怎么拼呢？也是经验论吗？
01:46:10	匡鸿深:	跟软件的插件一样，根据功能自由组合不同模块么
01:46:13	王健:	成块好处哪里啊
01:48:23	齐鹏飞:	global average pooling 现在可以替代fc了吗？
01:50:54	qinzongding:	第四点不清楚，能再讲一下吗
01:51:58	樊建昌:	老师，您说的是多模型融合吗》
01:52:05	樊建昌:	嗯嗯
01:52:09	华涓 陈:	随意拼接，可是size不一样啊
01:52:12	齐鹏飞:	控制不同的Kernel的输出size相同，是用padding来控制吗
01:52:35	张天虎:	不同长宽的的特征图怎么拼接呢
01:52:45	樊建昌:	要保证size一样
01:52:57	樊建昌:	图像宽高一样就行
01:53:10	樊建昌:	过
01:53:26	Eric:	既然要拼接，为什么不直接多个Kernel呢？
01:53:29	齐鹏飞:	weighted loss的权重怎么定呢
01:54:50	樊建昌:	都是大佬啊
01:54:51	王健:	facebook
01:55:04	张磊:	又一大神
01:55:08	Brian:	厉害
01:55:47	王健:	kaiming initialization
01:55:56	Eric:	虚线是什么意思？
01:56:25	罗家伟:	跨过去的箭头什么意思
01:56:28	潘力:	中间拱形箭头是什么意思？
01:56:30	qinzongding:	／2 是啥？
01:56:40	樊建昌:	箭头就是拼接
01:56:43	严广宇:	skip connection
01:57:45	王健:	短路吧
01:58:04	陈洪剑:	新的输入有上次的有残差信息
01:58:13	王健:	深度残差网络
01:58:14	陈洪剑:	残留
01:58:30	陈洪剑:	好像是这么说
01:58:39	严广宇:	residual network
01:58:43	Brian:	残差好像是
01:59:26	王健:	大神来打破天花板
02:01:07	Brian:	1
02:01:08	严广宇:	1
02:01:08	王健:	1
02:01:08	张磊:	1
02:01:08	陈智杰:	可以
02:01:09	唐陈:	1
02:01:09	潘力:	1
02:01:09	樊建昌:	1
02:01:11	刘向宏:	1
02:01:12	王健:	1
02:01:28	张磊:	挺流畅的
02:01:34	王健:	1
02:01:36	陈智杰:	可以
02:03:36	王健:	就是数学相加呗
02:03:43	王健:	像素值相加
02:04:43	王健:	总是1点几
02:05:20	齐鹏飞:	不会梯度爆炸吗？1.几？
02:05:36	罗家伟:	BP
02:05:56	樊建昌:	老师，为啥不会梯度爆炸呢
02:06:58	Eric:	pooling。
02:08:15	王健:	B分支可以调整对吗
02:08:18	陈智杰:	这种怎么会叫残差网络。。。感觉一点都不残啊
02:08:26	王健:	不是一成不变的
02:08:33	齐鹏飞:	f = h -x
02:08:40	Eric:	Caffe还可以用来画图吗？
02:08:41	匡鸿深:	做这步的操作就是为了得到那个1梯度值吗
02:09:21	樊建昌:	不是
02:09:38	樊建昌:	这个深层次原因有点复杂
02:11:16	Eric:	16
02:11:24	罗家伟:	150
02:11:28	马玄:	75
02:11:33	樊建昌:	50a
02:11:36	包颖:	48
02:11:39	夏敏:	48
02:12:43	张磊:	细节信息
02:13:07	Eric:	不懂，如果把3x3也改成64channel呢？
02:13:57	齐鹏飞:	少多少？
02:14:12	吴彬:	C
02:14:15	齐鹏飞:	ok
02:14:54	李林洲:	最后是有个全连接层吗
02:15:01	罗家伟:	有没可能把ResNet也模块化
02:15:15	王健:	是啊
02:15:31	Bruce:	average pool，听漏了
02:15:35	Bruce:	再介绍一下
02:16:12	Brian:	链接论文吗？
02:17:29	Eric:	哦哦，上万层有人试过吗？
02:17:52	houzhaoding:	为啥要变宽
02:19:46	匡鸿深:	resnet提出来有理论指导的吗？还是试验出来的
02:19:47	罗家伟:	Resnet只有一层ave pool？
02:19:51	何川:	圆圈是表示相加吗
02:19:55	齐鹏飞:	resnext 结构和group conv有区别吗
02:20:49	王健:	不喜欢啊
02:21:10	王健:	是不是计算量会特别大啊
02:23:43	王健:	这个有用的吗
02:23:57	张磊:	哈哈
02:24:00	王健:	基本全是resnet
02:24:04	张磊:	强势植入
02:24:09	齐鹏飞:	速度太差吧主要是
02:24:29	王健:	VGG INCEPTION RESNET
02:24:31	李文艺:	densenet121
02:24:53	WW BB:	10:10
02:25:24	邓 奇:	如果层数越多达到1000层，这样不是越容易过拟吗？这样意义在哪里呢？
02:25:26	houzhaoding:	老师这些需要掌握到啥程度？用的时候是直接在框架中搭建吗？
02:26:12	严广宇:	必会的程度，面试这是基础问题
02:26:30	Brian:	概念得搞清楚
02:26:33	潘力:	感觉真的是好复杂了
02:26:47	Brian:	理顺了还行，就是有点乱
02:26:52	匡鸿深:	有种打太极的感觉了
02:27:19	潘力:	怎么实现得了
02:27:20	齐鹏飞:	主要是很多地方前后的理论有点矛盾，最终都是归结于实验
02:27:34	houzhaoding:	有种云里雾里的感觉
02:28:30	何川:	inception net和resnet 的其它版本呢，比如说有inception V1，2，3，4，resnet也有，這些大概有哪些升级
02:29:03	齐鹏飞:	估计老师没时间讲
02:29:44	严广宇:	1
02:31:18	王健:	pytorch-version有吧
02:32:06	何川:	刚讲的是哪个版本的啊
02:32:09	王健:	老师通常你用这些网络怎么调用呢
02:32:40	Eric:	有人把SVM跟CNN结合吗？
02:33:10	王健:	一个是机器学习，一个是深度学习啊啊
02:33:25	张磊:	R-CNN
02:33:26	齐鹏飞:	有
02:33:27	Eric:	哦哦，谢谢。
02:33:28	张磊:	就是啊
02:33:31	徐楠:	rcnn最后的分类就是用的SVM
02:34:03	李文艺:	怎么移植到手机上呢
02:34:15	卢宇宙:	单片机...
02:34:26	齐鹏飞:	训好了download就行
02:35:04	王健:	好像mobilenet很有名
02:35:15	严广宇:	毕竟谷歌亲儿子
02:39:37	Eric:	这个歌
02:39:45	严广宇:	哈哈
02:39:57	严广宇:	输入法背锅
02:40:12	Eric:	这个跟知识蒸馏是两个思路吗？
02:40:15	Eric:	不好意思……
02:41:20	Eric:	哦哦。
02:43:10	罗家伟:	1
02:43:11	李文艺:	1
02:43:12	匡鸿深:	1
02:43:12	qinzongding:	111
02:43:12	樊建昌:	1
02:43:13	刘向宏:	1
02:43:14	齐鹏飞:	1
02:43:14	wanggan:	1
02:43:14	樊建昌:	1
02:43:15	张磊:	1
02:43:18	Brian:	F ,k  是什么？
02:43:27	樊建昌:	尺寸
02:43:31	houzhaoding:	kernel 为啥比原图多了一维
02:43:52	李文艺:	输出size
02:44:17	李文艺:	或者是输出channel
02:44:23	齐鹏飞:	砍去channelwise 的weights?
02:44:25	刘向宏:	不同的kernel，相当于不同的filter，和图像卷积后得到不同的信息
02:46:01	张磊:	拆开了
02:47:55	严广宇:	1
02:48:15	刘向宏:	1
02:48:21	樊建昌:	1
02:48:22	王健:	2
02:48:31	严广宇:	前几天笔试题碰到了
02:48:39	樊建昌:	牛掰牛掰
02:48:50	张磊:	你是入的商汤么？
02:49:02	张培栋:	网易?
02:49:08	张磊:	严？
02:49:24	严广宇:	不是，大疆
02:49:30	张磊:	6啊
02:49:34	严广宇:	划水的
02:49:42	樊建昌:	谦虚了啊
02:49:51	樊建昌:	广宇很强的
02:49:53	刘向宏:	膜拜
02:50:05	潘力:	666
02:50:58	张磊:	哈哈
02:51:00	樊建昌:	1
02:51:02	张磊:	那我满分
02:51:36	Eric:	BN是什么呀？
02:51:44	樊建昌:	老师讲了呀
02:51:48	Brian:	Batch NOrm
02:51:58	Eric:	哦哦，谢谢。
02:53:04	樊建昌:	损失函数？
02:53:08	regulus:	还有多久结束
02:54:19	齐鹏飞:	mobilenet 是基于什么网络改的呢
02:58:13	王奕楷:	两种解释对比强烈
02:58:19	陈智杰:	relu不是当激活函数用吗？那不用relu，是指不要激活函数了？
02:58:22	张磊:	哈哈
02:58:24	马一民:	MOI=？
02:58:31	张磊:	品质是臻品质
02:59:07	王健:	稍微说一下呗
03:00:02	王健:	就是增加参数呗
03:00:47	王健:	v2
03:01:43	wuxi:	老师，去掉RELU怎么保证非线性呢？复杂网络结构出来的非线性？
03:02:16	罗家伟:	两个非线性相加
03:02:24	wuxi:	soga
03:05:22	齐鹏飞:	加一个1*1卷积不行吗
03:08:51	Eric:	blob是什么？
03:09:14	齐鹏飞:	caffe里的数据层？
03:12:32	Eric:	elwise不是很适合并行吗？
03:18:27	Eric:	比如说哪些呀？
03:19:51	Eric:	卷积有优化吗？
03:22:18	Eric:	卷积运算有优化吗？
03:22:23	李文艺:	轻量级模型怎么移植到手机端呢
03:22:40	齐鹏飞:	octave conv?
03:22:57	Eric:	没听清楚，venograd？
03:23:12	龙云淋:	winograd
03:24:30	齐鹏飞:	shuffle 是为了混合channel的话，可不可以直接用1*1的卷积混合？
03:24:31	Eric:	哦哦，用并行做乘法
03:24:53	李文艺:	手机端可以直接用python去运行模型？
03:24:55	Eric:	目的不同，
03:25:13	Eric:	两个是独立的，
03:25:35	Eric:	如果用1x1，就不需要shuffle了。
03:25:38	龙云淋:	ncnn snpe了解下
03:26:10	邓 奇:	FLOPs只是时间复杂度的计算吗？
03:26:12	Eric:	更新的网络能列举一下名词吗？
03:26:45	Eric:	就是19年的，
03:26:54	Eric:	哦哦。
03:27:03	Eric:	谢谢。
03:27:20	Brian:	新课是您讲吗
03:27:23	Brian:	paddle
03:27:36	刘向宏:	谢谢老师
03:27:49	马一民:	今天听完，感觉模型都是大神整好的，逻辑就是通过训练搞出kennel。所以效率不是关注训练时的，是实际一个图片出来，要算出来结果够快？这个理解对吗？
03:28:01	龙云淋:	谢谢老师
03:28:27	马一民:	所以训练数量够大很重要？
03:28:42	Eric:	轻量型网络是在手机上面训练吗？
03:28:46	马一民:	除了pk模型，就是pk数据量啦？
03:28:52	齐鹏飞:	fpn还会再讲吗
03:29:46	houzhaoding:	到CNN这块有种基本听不懂的感觉。😭
03:29:59	Eric:	轻量型网络是在手机上面训练吗？
03:30:16	齐鹏飞:	不是
03:30:29	Brian:	需要多时间消化
03:30:31	齐鹏飞:	前向用就好
03:31:11	孟明:	老师有哪些比较好的硬件加速的模型？
03:31:13	马一民:	哦 训练集小 模型靠谱 效果也能出来吗？
03:31:26	Eric:	知识蒸馏是做什么，能介绍一下概念吗？
03:31:44	Eric:	哦哦。
03:32:43	高格格:	老师再见！
03:32:47	Brian:	晚安
03:32:51	潘力:	好的，谢谢老师！
03:32:53	Eric:	Kernel的设置属于超参数吗？
03:32:54	wanggan:	老师再见。
03:33:01	徐楠:	老师再见
